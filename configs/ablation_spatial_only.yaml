# ============================================================================
# ABLATION: SPATIAL ENCODING ONLY
# BLIP-2 + Scene Reasoning Module (Spatial Encoding, NO Relation Attention)
# ============================================================================
# Purpose: Isolate the contribution of 2D Spatial Position Encoding
#          independent of the Relation-Aware Attention mechanism.
#
# What's ENABLED:
#   ✓ Scene Reasoning Module (processing layers active)
#   ✓ Spatial Position Encoding (2D row/column embeddings)
#   ✓ Standard Self-Attention (without relative position bias)
#
# What's DISABLED:
#   ✗ Relation-Aware Attention (no position_bias in attention)
#
# Hypothesis: Spatial encoding alone should improve performance on
#             spatial reasoning questions by providing explicit 2D
#             position information to the model.
#
# Expected Behavior:
#   - Features receive 2D position embeddings
#   - Attention operates without relative position bias
#   - Performance should be between baseline and full proposed model
#   - May show improvement on spatial/counting questions
#
# Research Question: Does explicit 2D position encoding contribute
#                    to improved spatial understanding in VQA?
#
# Usage:
#   python scripts/train.py --config configs/ablation_spatial_only.yaml
#   python scripts/train.py --config configs/ablation_spatial_only.yaml --smoke_test
# ============================================================================

seed: 42

# Data Configuration
data:
  dataset_name: "HuggingFaceM4/VQAv2"
  train_split: "train"
  val_split: "validation"
  max_train_samples: null
  max_val_samples: null
  image_size: 224
  max_question_length: 32
  max_answer_length: 16
  num_workers: 2
  pin_memory: true
  prompt_template: "Question: {question} Answer:"
  cache_dir: null

# Model Configuration
model:
  model_name: "Salesforce/blip2-opt-2.7b"
  torch_dtype: "float16"
  
  # Freezing (identical to proposed for fair comparison)
  freeze_vision_encoder: true
  freeze_llm: true
  freeze_qformer: false
  
  # Scene Reasoning - ABLATION: Spatial Only
  use_scene_reasoning: true
  scene_hidden_dim: 768
  scene_num_heads: 8
  scene_num_layers: 2
  scene_mlp_ratio: 4.0
  scene_dropout: 0.1
  use_spatial_encoding: true     # ✓ ENABLED - Testing spatial contribution
  use_relation_attention: false  # ✗ DISABLED - No position bias in attention
  spatial_encoding_dim: 64
  
  # Generation (identical to proposed)
  max_new_tokens: 16
  num_beams: 3
  do_sample: false
  temperature: 1.0

# Training Configuration (MUST match proposed.yaml for fair comparison)
training:
  batch_size: 1
  gradient_accumulation_steps: 8
  effective_batch_size: 8
  
  learning_rate: 1.0e-5
  weight_decay: 0.01
  max_grad_norm: 1.0
  
  num_epochs: 3
  max_steps: null
  warmup_ratio: 0.1
  lr_scheduler_type: "cosine"
  
  fp16: true
  bf16: false
  gradient_checkpointing: false
  
  device: "auto"
  
  save_strategy: "epoch"
  save_steps: 500
  save_total_limit: 3
  resume_from_checkpoint: null
  
  eval_strategy: "epoch"
  eval_steps: 500
  
  early_stopping: false
  early_stopping_patience: 3
  
  smoke_test: false
  smoke_test_samples: 32
  smoke_test_steps: 5

# Logging Configuration
logging:
  output_dir: "./outputs"
  experiment_name: "ablation_spatial_only"
  use_tensorboard: true
  use_wandb: false
  wandb_project: "vlm-vqa-research"
  wandb_entity: null
  log_every_n_steps: 10
  save_predictions: true
  save_attention_maps: false  # No relation attention to visualize

# Evaluation Configuration
evaluation:
  compute_exact_match: true
  compute_normalized_match: true
  compute_vqa_accuracy: true
  save_error_analysis: true
  error_analysis_samples: 500
  output_csv: true
  output_json: true

# Runtime Configuration
runtime:
  execution_profile: "colab_train"
  sync_to_drive: false
  drive_output_path: "/content/drive/MyDrive/VLM_Thesis_Outputs"
  mac_dev_max_steps: 10
  mac_dev_max_samples: 50
  mac_dev_save_checkpoints: false
